# ğŸ•¸ï¸ Dead Link Web Scraper (in Go)

A simple, recursive dead link checker written in Go.

This project scrapes a given URL, recursively follows internal links, and logs any dead links. Designed as a learning project for exploring Go's core features like concurrency, HTTP/HTML handling, and database interaction.

> Built to practice Go through real-world scraping, concurrency, data persistence, and microservice architecture.

## ğŸš€ Features

- Accepts a starting URL to begin scraping.
- Recursively follows and checks links **within the same base domain**.
- Detects dead links:
  - Links that return 4xx or 5xx HTTP status codes.
  - Links that timeout.
- Logs all dead links to the console.
- Skips already visited URLs to prevent reprocessing.
- Handles redirects properly (3xx responses).
- Avoids infinite recursion or loops.

### Project Overview
The project is planned to evolve in **three stages**:

- Part 1: CLI Link Scanner
  - Basic recursive scanner using `net/http`, `x/net/html`, and concurrency
- Part 2: Storage and Persistent REST API endpoints
  - Adds PostgreSQL storage and REST endpoints
- Part 3: Scalable Microservice Backend
  - Queue-based job processing, gRPC, observability and async architecture

See [`progress.md`](./progress.md) for development breakdown.

## ğŸ§ª Example

Try scraping this test site:  
ğŸ”— [`https://scrape-me.dreamsofcode.io`](https://scrape-me.dreamsofcode.io)

## âš™ï¸ How It Works

- Uses the standard `net/http` package for HTTP requests.
- Parses HTML using `golang.org/x/net/html`.
- Maintains a set of visited URLs to avoid rechecking.
- Only scrapes pages within the base domain, but **does** validate external links without crawling them.
- Designed to be simple and extensible.

## ğŸ§  Design Considerations

### âœ… Handled Edge Cases

- **Redirects:** Follows 3xx redirects and treats them as part of the request lifecycle.
- **Infinite Recursion:** Keeps track of visited URLs to prevent loops.
- **Base Domain Limiting:** Recursively scans only within the original domain; external links are checked but not scraped.

### âŒ Not Yet Supported

- **JavaScript-rendered sites:** These require a headless browser (e.g. Playwright for Go). Could be added as an expansion.
- **Robots.txt or rate limiting:** Not yet respected. Use with caution on real websites.

## ğŸ§µ Potential Improvements

- âœ… Add concurrency with goroutines and channels for faster scanning.
- â± Timeout handling per request.
- ğŸ“„ Save results to a file (e.g. JSON or CSV).
- ğŸŒ Proxy support and user-agent randomization.
- ğŸ§ª Unit tests and structured logging.

## ğŸ› ï¸ Tech Stack

- [Go](https://golang.org/)
- [`net/http`](https://pkg.go.dev/net/http)
- [`x/net/html`](https://pkg.go.dev/golang.org/x/net/html)

## ğŸ” Next Steps
- **[Continue to Part 2 â€“ Persistent Dead Link Monitor](./part2.md)**
- **[Part 3 â€“ Scalable Media Service Architecture](./part3.md)**
- **[`progress.md`](./progress.md) â€“ Feature checklist and roadmap**

**multi-part case study** for portfolio or CV:

- **Part 1**: CLI Tool â€“ Go concurrency, parsing, scraping
- **Part 2**: API Backend â€“ Database modeling, HTTP API, Docker
- **Part 3**: Scalable Microservice â€“ gRPC, Kafka, observability


## Screenshots / Example Output

```bash
$ go run cmd/main.go https://example.com

âœ… https://example.com/about
âŒ https://example.com/dead-link (404)
â³ https://example.com/stuck (timeout)

Scan complete. 13 OK, 2 Dead Links.
```